{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10bc193-8712-483f-b0d6-26cf4c52974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import glob\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28caf51-75ca-410a-88d3-7ea9031dee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(\"../data/ember2018/tra*\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_json(filename,lines=True)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f3fbd2-6dac-4512-aa08-a14fac635a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_json(jsonl):\n",
    "    norm = {\n",
    "        'size': [],\n",
    "        'entropy' : [],\n",
    "        'vsize' : [],\n",
    "        'props' : \"\"\n",
    "    }\n",
    "    for obj in jsonl:\n",
    "        norm['size'].append(obj['size'])\n",
    "        norm['entropy'].append(obj['entropy'])\n",
    "        norm['vsize'].append(obj['vsize'])\n",
    "        props = \" \".join(obj['props'])+\" \"\n",
    "        norm['props']+=props\n",
    "    return norm\n",
    "\n",
    "def remove_infrequent(string,top_entries):\n",
    "    if string not in top_entries:\n",
    "        return 'other'\n",
    "    else:\n",
    "        if string == \"   \" or string == \"\":\n",
    "            string=\"[blank]\"\n",
    "        return string.strip()\n",
    "    \n",
    "def explode_section(df):\n",
    "    df_section = pd.json_normalize(df['section']).add_prefix(\"section.\")\n",
    "    df_section['section.sections'] = df_section['section.sections'].apply(list_to_json)\n",
    "    top_entries = df_section['section.entry'].value_counts()[:10].index.to_list()\n",
    "    df_section['section.entry'] = df_section['section.entry'].apply(remove_infrequent,top_entries=top_entries)\n",
    "    df_section = pd.get_dummies(df_section,prefix='section.entry',prefix_sep=\".\",columns=['section.entry'])\n",
    "    df_section = pd.concat([df_section,pd.json_normalize(df_section['section.sections']).add_prefix('section.sections.')],axis=1).drop(columns=['section.sections'])\n",
    "    df_section = explode_cols_list(df_section,\"section.sections.size\")\n",
    "    df_section = explode_cols_list(df_section,\"section.sections.entropy\")\n",
    "    df_section = explode_cols_list(df_section,\"section.sections.vsize\")\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df_section['section.sections.props'])\n",
    "    columns = ['section.sections.props.' + feature for feature in vectorizer.get_feature_names()]\n",
    "    df_props = pd.DataFrame(data=X.toarray(), columns=columns).astype('int8')\n",
    "    return pd.concat([df,df_section,df_props],axis=1).drop(['section.sections.props','section'],axis=1)\n",
    "\n",
    "def imports_one_hot(df, max_features=80):\n",
    "    imports = []\n",
    "    for i in range(df.shape[0]):\n",
    "        sentence = []\n",
    "        for key in df['imports'][i].keys():  # create list of lists of sentences for each elements\n",
    "            sentence.append(' '.join(df['imports'][i][key]))\n",
    "        sentence = ' '.join(sentence)  # create sentece of each list\n",
    "        sentence = re.sub(r\"[^a-zA-Z0-9 ]+\", '', sentence)  # remove special characters\n",
    "        imports.append(re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", sentence))  # put blank spaces between words\n",
    "        \n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform(imports)  # perform One-Hot in the most frequent words\n",
    "    \n",
    "    columns = ['imports_' + feature for feature in vectorizer.get_feature_names()]\n",
    "    df_imports = pd.DataFrame(data=X.toarray(), columns=columns).astype('int8')\n",
    "    df = pd.concat([df,df_imports], axis=1).drop(['imports'], axis=1)  # create new DataFrame\n",
    "    return df\n",
    "\n",
    "def exports_one_hot(df, max_features=40):\n",
    "    exports = []\n",
    "    for i in range(df.shape[0]):\n",
    "        sentence = df['exports'][i]\n",
    "        sentence = ' '.join(sentence)  # create sentece of each element\n",
    "        sentence = re.sub(r\"[^a-zA-Z0-9 ]+\", '', sentence)  # remove special characters\n",
    "        exports.append(re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", sentence))  # put blank spaces between words\n",
    "\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform(exports)  # perform One-Hot in the most frequent words\n",
    "\n",
    "    columns = ['exports_' + feature for feature in vectorizer.get_feature_names()]\n",
    "    df_exports = pd.DataFrame(data=X.toarray(), columns=columns).astype('int8')\n",
    "    df = pd.concat([df,df_exports], axis=1).drop(['exports'], axis=1)  # create new DataFrame\n",
    "    return df\n",
    "\n",
    "def explode_cols_list(df,colname):\n",
    "#     max_len_list = df[colname].apply(len).values.max()\n",
    "#     newcols = [colname+\"_\"+str(i) for i in range(max_len_list)]\n",
    "#     df[newcols] = pd.DataFrame(df[colname].tolist(), index= df.index)\n",
    "#     df = df.drop(columns=[colname])\n",
    "    df[colname+\".mean\"] = df[colname].apply(np.mean)\n",
    "    df[colname+\".std\"] = df[colname].apply(np.std)\n",
    "    df[colname+'.median'] = df[colname].apply(np.median)\n",
    "    df[colname+'.max'] = df[colname].apply(np.max,initial=0)\n",
    "    df[colname+'.min'] = df[colname].apply(np.min,initial=0)\n",
    "    df[colname+'.var'] = df[colname].apply(np.var)\n",
    "#     df[colname+'.mode'] = df[colname].apply(stats.mode)\n",
    "    df = df.drop(columns=[colname])\n",
    "    return df\n",
    "\n",
    "def explode_header(df):\n",
    "    df_header = pd.json_normalize(df.header)\n",
    "    new_cols = ['header.'+col for col in df_header.columns]\n",
    "    df_header.rename(mapper=dict(zip(df_header.columns,new_cols)),axis=1, inplace=True)\n",
    "    one_hot_cols = ['header.coff.machine','header.optional.subsystem','header.optional.magic']\n",
    "    df_header = pd.get_dummies(df_header,prefix=one_hot_cols,prefix_sep='.',columns=one_hot_cols)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_header = df_header.join(pd.DataFrame(mlb.fit_transform(df_header.pop('header.coff.characteristics')),columns=['header.coff.characteristics.'+classe  for classe in mlb.classes_],index=df_header.index))\n",
    "    df_header = df_header.join(pd.DataFrame(mlb.fit_transform(df_header.pop('header.optional.dll_characteristics')),columns=['header.optional.dll_characteristics.'+classe  for classe in mlb.classes_],index=df_header.index))\n",
    "    df = df.drop(columns=['header'])\n",
    "    return pd.concat([df, df_header], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c021f-1e26-4501-b0d6-8415e9a69644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(columns=['sha256','md5','appeared'])\n",
    "df = explode_section(df)\n",
    "df = explode_cols_list(df,'histogram')\n",
    "df = explode_cols_list(df,'byteentropy')\n",
    "df = explode_header(df)\n",
    "df = pd.concat([df, pd.json_normalize(df['strings']).add_prefix(\"strings.\"), pd.json_normalize(df['general']).add_prefix(\"general.\")], axis=1)\n",
    "df = explode_cols_list(df,'strings.printabledist')\n",
    "df = imports_one_hot(df)\n",
    "df = exports_one_hot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2fe4af-0962-4788-8626-99b03f8dc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"final_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5900f4b-5bc3-4997-962e-5ab90a5a7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
